# ec2 실행 
su hadoop 



# hadoop 구동 
cd ~hadoop
. start.sh


# pyspark notebook 구동 
pyspark --master yarn --num-executors 3

# 공유된 2개의 csv 파일을 
# hdfs에 업로드 
hdfs dfs -put 파일명 /data/
------------------------
lines = sc.textFile("hdfs:///data/1800.csv")

def parseLine(line):
    fields = line.split(',')
    stationID = fields[0]
    entryType = fields[2]
    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0
    return (stationID, entryType, temperature)

parsedLines = lines.map(parseLine)
minTemps = parsedLines.filter(lambda x: "TMIN" in x[1])

stationTemps = minTemps.map(lambda x: (x[0], x[2]))
minTemps = stationTemps.reduceByKey(lambda x, y: min(x,y))
results = minTemps.collect();


# FP -> Pure Function(사이드 이펙트가 없는 함수)
results = parsedLines.filter(lambda x: "TMIN" in x[1])\
            .map(lambda x: (x[0], x[2]))\
            .reduceByKey(lambda x, y: min(x,y))\
            .collect()
            
            
# DataFrame
from pyspark.sql import SparkSession
from pyspark.sql import functions as func
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType

schema = StructType([ \
                     StructField("stationID", StringType(), True), \
                     StructField("date", IntegerType(), True), \
                     StructField("measure_type", StringType(), True), \
                     StructField("temperature", FloatType(), True)])

df = spark.read.schema(schema).csv("hdfs:///data/1800.csv")                     
minTemps = df.filter(df.measure_type == "TMIN")
stationTemps = minTemps.select("stationID", "temperature")
minTempsByStation = stationTemps.groupby('stationID').min('temperature')        

minTempsByStationF = minTempsByStation.withColumn("temperature",
                                                  func.round(func.col("min(temperature)") * 0.1 * (9.0 / 5.0) + 32.0, 2))\
                                                  .select("stationID", "temperature").sort("temperature")
                  
rt = df.filter(df.measure_type == "TMIN")\
    .select("stationID", "temperature")\
    .groupby('stationID').min('temperature')\
    .withColumn("temperature",  func.round(func.col("min(temperature)") * 0.1 * (9.0 / 5.0) + 32.0, 2))\
    .select("stationID", "temperature").sort("temperature")

--------------------------------------------
input = sc.textFile("hdfs:///data/customer-orders.csv")
def extractCustomerPricePairs(line):
    fields = line.split(',')
    return (int(fields[0]), float(fields[2]))

# value 값으로 정렬 
result =     input.map(extractCustomerPricePairs)\
    				.reduceByKey(lambda x, y: x + y)\
            .map(lambda x : (x[1], x[0]))\
            .sortByKey()
            
# key값으로정렬            
result =     input.map(extractCustomerPricePairs)\
    				.reduceByKey(lambda x, y: x + y)\
            .sortByKey()    
     
     

DataFrame으로 처리 
------------------
from pyspark.sql import SparkSession
from pyspark.sql import functions as func
from pyspark.sql.types import StructType, StructField, IntegerType, FloatType

customerOrderSchema = StructType([ \
                                  StructField("cust_id", IntegerType(), True),
                                  StructField("item_id", IntegerType(), True),
                                  StructField("amount_spent", FloatType(), True)
                                  ])
customDF = spark.read.schema(customerOrderSchema).csv("hdfs:///data/customer-orders.csv")
rt =    customDF.groupby('cust_id').agg( func.round(func.sum("amount_spent"), 2).alias('total_spent'))\
        .sort('total_spent')
rt.show()


